{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movement\n",
      "No Significance       1013\n",
      "Mid-Sized Upward       111\n",
      "Mid-Sized Downward      94\n",
      "Large Upward            20\n",
      "Large Downward          14\n",
      "Name: count, dtype: int64\n",
      "Index(['Day_1 Ticker Open', 'Day_1 Ticker High', 'Day_1 Ticker Low',\n",
      "       'Day_1 Ticker Adj Close', 'Day_1 Ticker Volume',\n",
      "       'Day_1 Ticker Percent Change', 'Day_1 SP500 Open', 'Day_1 SP500 High',\n",
      "       'Day_1 SP500 Low', 'Day_1 SP500 Adj Close', 'Day_1 SP500 Volume',\n",
      "       'Day_1 SP500 Percent Change', 'Day_1 TM Open', 'Day_1 TM High',\n",
      "       'Day_1 TM Low', 'Day_1 TM Adj Close', 'Day_1 TM Volume',\n",
      "       'Day_1 TM Percent Change', 'Day_2 Ticker Open', 'Day_2 Ticker High',\n",
      "       'Day_2 Ticker Low', 'Day_2 Ticker Adj Close', 'Day_2 Ticker Volume',\n",
      "       'Day_2 Ticker Percent Change', 'Day_2 SP500 Open', 'Day_2 SP500 High',\n",
      "       'Day_2 SP500 Low', 'Day_2 SP500 Adj Close', 'Day_2 SP500 Volume',\n",
      "       'Day_2 SP500 Percent Change', 'Day_2 TM Open', 'Day_2 TM High',\n",
      "       'Day_2 TM Low', 'Day_2 TM Adj Close', 'Day_2 TM Volume',\n",
      "       'Day_2 TM Percent Change', 'Day_3 Ticker Open', 'Day_3 Ticker High',\n",
      "       'Day_3 Ticker Low', 'Day_3 Ticker Adj Close', 'Day_3 Ticker Volume',\n",
      "       'Day_3 Ticker Percent Change', 'Day_3 SP500 Open', 'Day_3 SP500 High',\n",
      "       'Day_3 SP500 Low', 'Day_3 SP500 Adj Close', 'Day_3 SP500 Volume',\n",
      "       'Day_3 SP500 Percent Change', 'Day_3 TM Open', 'Day_3 TM High',\n",
      "       'Day_3 TM Low', 'Day_3 TM Adj Close', 'Day_3 TM Volume',\n",
      "       'Day_3 TM Percent Change', 'Day_4 Ticker Open', 'Day_4 Ticker High',\n",
      "       'Day_4 Ticker Low', 'Day_4 Ticker Adj Close', 'Day_4 Ticker Volume',\n",
      "       'Day_4 Ticker Percent Change', 'Day_4 SP500 Open', 'Day_4 SP500 High',\n",
      "       'Day_4 SP500 Low', 'Day_4 SP500 Adj Close', 'Day_4 SP500 Volume',\n",
      "       'Day_4 SP500 Percent Change', 'Day_4 TM Open', 'Day_4 TM High',\n",
      "       'Day_4 TM Low', 'Day_4 TM Adj Close', 'Day_4 TM Volume',\n",
      "       'Day_4 TM Percent Change', 'Day_5 Ticker Open', 'Day_5 Ticker High',\n",
      "       'Day_5 Ticker Low', 'Day_5 Ticker Adj Close', 'Day_5 Ticker Volume',\n",
      "       'Day_5 Ticker Percent Change', 'Day_5 SP500 Open', 'Day_5 SP500 High',\n",
      "       'Day_5 SP500 Low', 'Day_5 SP500 Adj Close', 'Day_5 SP500 Volume',\n",
      "       'Day_5 SP500 Percent Change', 'Day_5 TM Open', 'Day_5 TM High',\n",
      "       'Day_5 TM Low', 'Day_5 TM Adj Close', 'Day_5 TM Volume',\n",
      "       'Day_5 TM Percent Change', 'Movement_Large Downward',\n",
      "       'Movement_Large Upward', 'Movement_Mid-Sized Downward',\n",
      "       'Movement_Mid-Sized Upward', 'Movement_No Significance'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Valid Time Periods: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "\n",
    "data_ticker = yf.download(\"XOM\", period=\"5y\")\n",
    "data_SP500 = yf.download(\"SPY\", period=\"5y\")\n",
    "data_TM = yf.download(\"VTI\", period=\"5y\")\n",
    "\n",
    "data_ticker[\"Percent Change\"] = data_ticker[\"Close\"].pct_change() * 100\n",
    "data_ticker = data_ticker.dropna(subset=[\"Percent Change\"])\n",
    "\n",
    "data_SP500['Percent Change'] = data_SP500['Close'].pct_change() * 100\n",
    "data_SP500 = data_SP500.dropna(subset=['Percent Change'])\n",
    "\n",
    "data_TM['Percent Change'] = data_TM['Close'].pct_change() * 100\n",
    "data_TM = data_TM.dropna(subset=['Percent Change'])\n",
    "\n",
    "n_days = 5\n",
    "candlestick_data_10_days = pd.DataFrame()\n",
    "\n",
    "for i in range(1, n_days + 1):\n",
    "    candlestick_data_10_days[f\"Day_{i} Ticker Open\"] = data_ticker[\"Open\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} Ticker High\"] = data_ticker[\"High\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} Ticker Low\"] = data_ticker[\"Low\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} Ticker Adj Close\"] = data_ticker[\"Adj Close\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} Ticker Volume\"] = data_ticker[\"Volume\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} Ticker Percent Change\"] = data_ticker[\"Percent Change\"].shift(i)\n",
    "\n",
    "    candlestick_data_10_days[f\"Day_{i} SP500 Open\"] = data_SP500[\"Open\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} SP500 High\"] = data_SP500[\"High\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} SP500 Low\"] = data_SP500[\"Low\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} SP500 Adj Close\"] = data_SP500[\"Adj Close\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} SP500 Volume\"] = data_SP500[\"Volume\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} SP500 Percent Change\"] = data_SP500[\"Percent Change\"].shift(i)\n",
    "\n",
    "    candlestick_data_10_days[f\"Day_{i} TM Open\"] = data_TM[\"Open\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} TM High\"] = data_TM[\"High\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} TM Low\"] = data_TM[\"Low\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} TM Adj Close\"] = data_TM[\"Adj Close\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} TM Volume\"] = data_TM[\"Volume\"].shift(i)\n",
    "    candlestick_data_10_days[f\"Day_{i} TM Percent Change\"] = data_TM[\"Percent Change\"].shift(i)\n",
    "\n",
    "def categorize_movement(change):\n",
    "    if change <= -5:\n",
    "        return \"Large Downward\"\n",
    "    elif change <= -2.5:\n",
    "        return \"Mid-Sized Downward\"\n",
    "    elif change >= 5:\n",
    "        return \"Large Upward\"\n",
    "    elif change >= 2.5:\n",
    "        return \"Mid-Sized Upward\"\n",
    "    else:\n",
    "        return \"No Significance\"\n",
    "\n",
    "candlestick_data_10_days[\"Movement\"] = data_ticker[\"Percent Change\"].apply(categorize_movement)\n",
    "candlestick_data_10_days = candlestick_data_10_days.dropna().reset_index(drop=True)\n",
    "\n",
    "frequency = candlestick_data_10_days[\"Movement\"].value_counts()\n",
    "print(frequency)\n",
    "\n",
    "candlestick_data_10_days_encoded = pd.get_dummies(candlestick_data_10_days, columns=[\"Movement\"])\n",
    "candlestick_data_10_days_encoded = candlestick_data_10_days_encoded.astype(int)\n",
    "\n",
    "print(candlestick_data_10_days_encoded.columns)\n",
    "classification_columns = ['Movement_Large Downward', \n",
    "                          'Movement_Large Upward', \n",
    "                          'Movement_Mid-Sized Downward',\n",
    "                          'Movement_Mid-Sized Upward', \n",
    "                          'Movement_No Significance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "test_size = 200\n",
    "train_df = candlestick_data_10_days_encoded.iloc[:-test_size]\n",
    "test_df = candlestick_data_10_days_encoded.iloc[-test_size:]\n",
    "\n",
    "x_train = train_df.drop(columns=classification_columns)\n",
    "y_train = train_df[classification_columns]\n",
    "x_test = test_df.drop(columns=classification_columns)\n",
    "y_test = test_df[classification_columns]\n",
    "\n",
    "feature_number = x_train.shape[1]\n",
    "print(feature_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.6260 - loss: 2084418.2500\n",
      "Epoch 2/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.6626 - loss: 840404.3125\n",
      "Epoch 3/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.6892 - loss: 729874.8125\n",
      "Epoch 4/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495us/step - accuracy: 0.5021 - loss: 530913.0625\n",
      "Epoch 5/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - accuracy: 0.6141 - loss: 342590.2188\n",
      "Epoch 6/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - accuracy: 0.6105 - loss: 263765.2188\n",
      "Epoch 7/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.6425 - loss: 261222.1719\n",
      "Epoch 8/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468us/step - accuracy: 0.6525 - loss: 257587.7812\n",
      "Epoch 9/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.5805 - loss: 201222.1406\n",
      "Epoch 10/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.5613 - loss: 224357.4062\n",
      "Epoch 11/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492us/step - accuracy: 0.6216 - loss: 170606.1719\n",
      "Epoch 12/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step - accuracy: 0.6705 - loss: 278114.1250\n",
      "Epoch 13/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.6325 - loss: 145504.8281\n",
      "Epoch 14/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.5273 - loss: 265513.4375\n",
      "Epoch 15/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6056 - loss: 154219.4531 \n",
      "Epoch 16/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.6219 - loss: 115487.8984\n",
      "Epoch 17/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.5646 - loss: 150711.7031\n",
      "Epoch 18/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - accuracy: 0.5032 - loss: 206132.7969\n",
      "Epoch 19/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604us/step - accuracy: 0.6079 - loss: 119215.6328\n",
      "Epoch 20/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.5480 - loss: 149398.9219\n",
      "Epoch 21/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step - accuracy: 0.6554 - loss: 117984.0547\n",
      "Epoch 22/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.6295 - loss: 97439.4531\n",
      "Epoch 23/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - accuracy: 0.6361 - loss: 87524.7500\n",
      "Epoch 24/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 470us/step - accuracy: 0.6566 - loss: 161126.0625\n",
      "Epoch 25/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.6338 - loss: 98952.9219\n",
      "Epoch 26/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - accuracy: 0.6520 - loss: 78463.6484\n",
      "Epoch 27/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - accuracy: 0.6099 - loss: 73162.3359\n",
      "Epoch 28/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 578us/step - accuracy: 0.6782 - loss: 94223.7891\n",
      "Epoch 29/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 600us/step - accuracy: 0.6207 - loss: 65430.0117\n",
      "Epoch 30/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - accuracy: 0.5880 - loss: 99340.5625\n",
      "Epoch 31/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 560us/step - accuracy: 0.6027 - loss: 104843.1719\n",
      "Epoch 32/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630us/step - accuracy: 0.6162 - loss: 97328.6250\n",
      "Epoch 33/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 603us/step - accuracy: 0.5611 - loss: 138133.2188\n",
      "Epoch 34/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.6171 - loss: 83850.1875\n",
      "Epoch 35/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - accuracy: 0.5908 - loss: 57460.6289\n",
      "Epoch 36/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 0.6755 - loss: 90888.3359\n",
      "Epoch 37/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - accuracy: 0.5998 - loss: 86881.7266\n",
      "Epoch 38/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595us/step - accuracy: 0.6698 - loss: 59370.9453\n",
      "Epoch 39/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.6684 - loss: 51340.5273\n",
      "Epoch 40/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - accuracy: 0.5436 - loss: 85417.9609\n",
      "Epoch 41/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.5522 - loss: 123635.2031\n",
      "Epoch 42/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 570us/step - accuracy: 0.6628 - loss: 73388.7812\n",
      "Epoch 43/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.6114 - loss: 63260.2539\n",
      "Epoch 44/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568us/step - accuracy: 0.6312 - loss: 61694.0625\n",
      "Epoch 45/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 546us/step - accuracy: 0.6398 - loss: 126675.7578\n",
      "Epoch 46/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605us/step - accuracy: 0.6129 - loss: 91672.2344\n",
      "Epoch 47/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - accuracy: 0.5702 - loss: 61519.7617\n",
      "Epoch 48/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.6725 - loss: 41963.8633\n",
      "Epoch 49/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.6438 - loss: 72377.5625\n",
      "Epoch 50/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 615us/step - accuracy: 0.6315 - loss: 42760.7188\n",
      "Epoch 51/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 0.6264 - loss: 37020.2617\n",
      "Epoch 52/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 614us/step - accuracy: 0.5441 - loss: 74483.1094\n",
      "Epoch 53/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - accuracy: 0.6239 - loss: 50871.6641\n",
      "Epoch 54/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.6291 - loss: 48960.3359\n",
      "Epoch 55/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - accuracy: 0.6228 - loss: 53338.5938\n",
      "Epoch 56/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6505 - loss: 45661.9609 \n",
      "Epoch 57/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.6169 - loss: 49703.7578\n",
      "Epoch 58/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.5998 - loss: 51305.4609\n",
      "Epoch 59/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step - accuracy: 0.5901 - loss: 46822.8164\n",
      "Epoch 60/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - accuracy: 0.6403 - loss: 77986.3438\n",
      "Epoch 61/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.6430 - loss: 53055.8008\n",
      "Epoch 62/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 0.6218 - loss: 41945.6367\n",
      "Epoch 63/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.5866 - loss: 42893.9141\n",
      "Epoch 64/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.6351 - loss: 31195.2520\n",
      "Epoch 65/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - accuracy: 0.5759 - loss: 49465.2734\n",
      "Epoch 66/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - accuracy: 0.6912 - loss: 65163.2070\n",
      "Epoch 67/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - accuracy: 0.5576 - loss: 71995.9219\n",
      "Epoch 68/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 543us/step - accuracy: 0.6486 - loss: 60832.5117\n",
      "Epoch 69/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 513us/step - accuracy: 0.6520 - loss: 47211.8320\n",
      "Epoch 70/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.6595 - loss: 39552.0742\n",
      "Epoch 71/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 561us/step - accuracy: 0.4362 - loss: 187310.6094\n",
      "Epoch 72/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 751us/step - accuracy: 0.5368 - loss: 98364.8359\n",
      "Epoch 73/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 0.4678 - loss: 120888.5625\n",
      "Epoch 74/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - accuracy: 0.5868 - loss: 60273.0078\n",
      "Epoch 75/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487us/step - accuracy: 0.6519 - loss: 37239.7773\n",
      "Epoch 76/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.6421 - loss: 64266.8633\n",
      "Epoch 77/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494us/step - accuracy: 0.6662 - loss: 33395.4727\n",
      "Epoch 78/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.6420 - loss: 32246.0449\n",
      "Epoch 79/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - accuracy: 0.6507 - loss: 22270.4180\n",
      "Epoch 80/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - accuracy: 0.6730 - loss: 33868.9688\n",
      "Epoch 81/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6291 - loss: 29681.9473\n",
      "Epoch 82/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - accuracy: 0.7008 - loss: 25097.1914\n",
      "Epoch 83/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 517us/step - accuracy: 0.6277 - loss: 23416.4473\n",
      "Epoch 84/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - accuracy: 0.6988 - loss: 22833.1895\n",
      "Epoch 85/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.5123 - loss: 97897.2344\n",
      "Epoch 86/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 533us/step - accuracy: 0.5848 - loss: 43341.5703\n",
      "Epoch 87/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 584us/step - accuracy: 0.6394 - loss: 40273.6602\n",
      "Epoch 88/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.6402 - loss: 27541.0684\n",
      "Epoch 89/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 577us/step - accuracy: 0.5261 - loss: 57614.9297\n",
      "Epoch 90/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.7091 - loss: 43683.4141\n",
      "Epoch 91/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.5565 - loss: 101233.3594\n",
      "Epoch 92/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 0.6195 - loss: 67650.4141\n",
      "Epoch 93/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488us/step - accuracy: 0.6463 - loss: 40321.4883\n",
      "Epoch 94/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 0.6870 - loss: 30797.9688\n",
      "Epoch 95/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4765 - loss: 71465.7422  \n",
      "Epoch 96/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476us/step - accuracy: 0.5688 - loss: 39372.4414\n",
      "Epoch 97/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.7126 - loss: 54020.2031\n",
      "Epoch 98/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480us/step - accuracy: 0.6352 - loss: 35318.3359\n",
      "Epoch 99/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - accuracy: 0.6634 - loss: 23792.6816\n",
      "Epoch 100/100\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.5257 - loss: 48392.5586\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 535us/step - accuracy: 0.9598 - loss: 8355.1504\n",
      "[9818.1923828125, 0.949999988079071]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "def my_model(num_neurons, num_features):\n",
    "    # Create a Sequential Neutral Network\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(num_features,)))\n",
    "    model.add(keras.layers.Dense(num_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(30, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(5, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(optimizer, loss=\"categorical_crossentropy\", metrics=[\"\"])\n",
    "    return model\n",
    "\n",
    "model = my_model(60, num_features=feature_number)\n",
    "model.fit(x_train, y_train, epochs = 100, batch_size=50, verbose = 1)\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x31e453ec0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Class 0:\n",
      "TP: 0, FP: 190, TN: 10, FN: 0\n",
      "\n",
      "Class 1:\n",
      "TP: 0, FP: 1, TN: 199, FN: 0\n",
      "\n",
      "Class 2:\n",
      "TP: 0, FP: 5, TN: 193, FN: 2\n",
      "\n",
      "Class 3:\n",
      "TP: 0, FP: 4, TN: 191, FN: 5\n",
      "\n",
      "Class 4:\n",
      "TP: 0, FP: 0, TN: 7, FN: 193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "# y_true_indices = [classification_columns.index(label) for label in y_true]\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "TP = {}\n",
    "FP = {}\n",
    "FN = {}\n",
    "TN = {}\n",
    "\n",
    "# Calculate the metrics for each class\n",
    "for i in range(len(cm)):\n",
    "    TP[i] = cm[i, i]\n",
    "    FP[i] = sum(cm[:, i]) - cm[i, i]\n",
    "    FN[i] = sum(cm[i, :]) - cm[i, i]\n",
    "    TN[i] = cm.sum() - (FP[i] + FN[i] + TP[i])\n",
    "\n",
    "# Output the results\n",
    "for i in range(len(cm)):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"TP: {TP[i]}, FP: {FP[i]}, TN: {TN[i]}, FN: {FN[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
